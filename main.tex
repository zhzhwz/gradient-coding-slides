\documentclass{beamer}

\usepackage{graphicx}
\usepackage{caption}

\mode<presentation>
{
  \usetheme{Darmstadt}

  \setbeamercovered{transparent}
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Gradient Coding}

\author{Runtian Zhu \\ \texttt{zhurt23@m.fudan.edu.cn}}

\date{2023.11.2}

\begin{document}

\captionsetup[figure]{labelformat=empty}

\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Outline}
%   \tableofcontents
% \end{frame}

\section{Gradient Coding}

\subsection{Reference}

\begin{frame}{Reference}

\begin{itemize}
    \item R. Tandon, Q. Lei, A. G. Dimakis, and N. Karampatziakis, “\textbf{Gradient coding: Avoiding stragglers in distributed learning},” in International Conference on Machine Learning, PMLR, 2017, pp. 3368–3376.
\end{itemize}

\begin{figure}
    \centering
    \begin{minipage}[t]{.2\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{res/Rashish Tandon.jpg}
        \caption{Rashish Tandon}
    \end{minipage}
    \begin{minipage}[t]{.2\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{res/Qi Lei.jpg}
        \caption{Qi Lei}
    \end{minipage}
    \begin{minipage}[t]{.2\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{res/alexdimakis_sm.jpg}
        \caption{Alexandros G. Dimakis}
    \end{minipage}
    \begin{minipage}[t]{.2\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{res/Nikos Karampatziakis.jpg}
        \caption{Nikos Karampatziakis}
    \end{minipage}
\end{figure}

\end{frame}

\begin{frame}
    \frametitle{Machine Learning: Regression}

    \begin{block}{Regression (fake)}
        Given a data set $D = \{(\boldsymbol{x}_1, \boldsymbol{y}_1), \dots, (\boldsymbol{x}_N, \boldsymbol{y}_N)\}$. The goal is to find a function $f$ from some function space $F$ such that $f(\boldsymbol{x}_i) = \boldsymbol{y}_i$.
    \end{block}

    \begin{example}
        \[D = \{(0, 0), (1, 1)\}\]
        \[F = \{x^2 + ax + b \vert a, b \in \mathbb{R}\}\]
        \[f(x) = x^2\]
    \end{example}
\end{frame}

\begin{frame}
    \frametitle{Machine Learning: Regression}

    \begin{block}{Regression (fake)}
        Given a data set $D = \{(\boldsymbol{x}_1, \boldsymbol{y}_1), \dots, (\boldsymbol{x}_N, \boldsymbol{y}_N)\}$. The goal is to find a function $f$ from some function space $F$ such that $f(\boldsymbol{x}_i) = \boldsymbol{y}_i$.
    \end{block}

    \begin{example}
        \[D = \{(0, 0), (1, 1), (-1, 0)\}\]
        \[F = \{x^2 + ax + b \vert a, b \in \mathbb{R}\}\]
        \[\emptyset\]
    \end{example}

    \begin{block}{Regression (informal)}
        Given a data set $D = \{(\boldsymbol{x}_1, \boldsymbol{y}_1), \dots, (\boldsymbol{x}_N, \boldsymbol{y}_N)\}$. The goal is to find a function $f$ from some function space $F$ such that $f$ fits $D$ best.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Machine Learning: Regression}

    \begin{block}{Regression}
        Given a data set $D = \{(\boldsymbol{x}_1, \boldsymbol{y}_1), \dots, (\boldsymbol{x}_N, \boldsymbol{y}_N)\}$. The goal is to find a function $f_{\hat{\boldsymbol{\theta}}}$ from some function space $F = \{f_{\boldsymbol{\theta}} \vert \boldsymbol{\theta} \in \mathbb{R}^m\}$ such that
        \[\hat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta}} L(\boldsymbol{\theta})\]
    \end{block}
    
    \begin{block}{Loss Function: Mean Squared Error (MSE)}
        \[L(\boldsymbol{\theta}) = \frac{1}{N}\sum_{i = 1}^{N} \lVert\boldsymbol{y}_i - f_{\boldsymbol{\theta}}(\boldsymbol{x}_i)\rVert^2\]
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Machine Learning: Regression}

    \[D = \{(0, 0), (1, 1), (-1, 0)\},\; F = \{x^2 + ax + b \vert a, b \in \mathbb{R}\}, \;\boldsymbol{\theta} = \begin{bmatrix}
        a \\
        b
    \end{bmatrix}\]
    \begin{align*}
        L(\boldsymbol{\theta}) &= \frac{(0 - f_{\boldsymbol{\theta}}(0))^2 + (1 - f_{\boldsymbol{\theta}}(1))^2 + (0 - f_{\boldsymbol{\theta}}(-1))^2}{3} \\
        &= \frac{2a^{2} + 3b^{2} - 2a + 2b + 1}{3}
    \end{align*}
    \[f = x^2 + \frac{1}{2}x - \frac{1}{3},\quad L(\boldsymbol{\theta}) = \frac{1}{18}\]
    \centering
    \includegraphics[height=.3\textheight]{res/function.png}

\end{frame}

\begin{frame}
    
\frametitle{Gradient Descent}
    
\begin{block}{Gradient Descent}
    Goal: to find a $\hat{\boldsymbol{\theta}}$ such that $\hat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta}} L(\boldsymbol{\theta})$.

    \[\boldsymbol{\theta}_{0} = random\,generated\]
    \[\boldsymbol{\theta}_{k + 1} = \boldsymbol{\theta}_{k} - \alpha\boldsymbol{\nabla} L(\boldsymbol{\theta}_{k})\]

    where $\alpha$ is chosen s.t. $\alpha > 0$.
\end{block}

\begin{example}
    \begin{columns}
        \begin{column}{.5\textwidth}
            \[L(x, y) = \frac{2x^{2} + 3y^{2} - 2x + 2y + 1}{3}\]
        \end{column}
        \begin{column}{.4\textwidth}
            \includegraphics[height=.3\textheight]{res/func3d.png}
        \end{column}
    \end{columns}
\end{example}

\end{frame}

\begin{frame}{Distributed Learning}{Gradient Descent in Machine Learning}

\begin{block}{Gradient Descent}
    \[\boldsymbol{\theta}_{k + 1} = \boldsymbol{\theta}_{k} - \alpha\boldsymbol{\nabla} L(\boldsymbol{\theta}_{k})\]
\end{block}
    
\begin{block}{Loss Function: Mean Squared Error (MSE)}
    \[L(\boldsymbol{\theta}) = \frac{1}{N}\sum_{i = 1}^{N} \lVert\boldsymbol{y}_i - f_{\boldsymbol{\theta}}(\boldsymbol{x}_i)\rVert^2\]
    \[\boldsymbol{\nabla}L(\boldsymbol{\theta}) = \sum_{i = 1}^{N}\frac{\partial\frac{1}{N}\lVert\boldsymbol{y}_i - f_{\boldsymbol{\theta}}(\boldsymbol{x}_i)\rVert^2}{\partial\boldsymbol{\theta}}\]
    Gradient can be calculated distributedly
\end{block}

\end{frame}

\begin{frame}{Distributed Learning}{Straggler Problem}

\begin{figure}
    \centering
    \includegraphics[height=.5\textheight]{res/distributed_learning.pdf}
\end{figure}

Problem:
\begin{itemize}
    \item Some workers may be stragglers (work much slower)
\end{itemize}

Possible Solutions:
\begin{itemize}
    \item Asynchronous SGD
    \item Synchronous minibatch SGD
\end{itemize}

\end{frame}

\section{Gradient Coding}

\subsection{An Example}

\begin{frame}{Distributed Learning}{Solution: replication}

\begin{figure}
    \centering
    \includegraphics[height=.7\textheight]{res/replicate.pdf}
    % \begin{minipage}[t]{0.48\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{res/replicate.pdf}
    %     \caption{Replication}
    %     \end{minipage}
    %     \begin{minipage}[t]{0.48\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{res/distributed_learning.pdf}
    %     \caption{Naive synchronous gradient descent}
    % \end{minipage}
\end{figure}

\end{frame}

\begin{frame}{Distributed Learning}{Gradient Coding}

\begin{figure}
    \centering
    \includegraphics[height=.7\textheight]{res/gradient coding.pdf}
    % \begin{minipage}[t]{0.48\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{res/gradient coding.pdf}
    %     \caption{Gradient coding}
    %     \end{minipage}
    %     \begin{minipage}[t]{0.48\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{res/replicate.pdf}
    %     \caption{Replication}
    % \end{minipage}
\end{figure}

\end{frame}

\subsection{General Case}

\begin{frame}{Gradient Coding}{The General Setup}

\begin{definition}
    Let $f$ denotes the number of combinations of surviving workers/non-stragglers, $n$ denotes the number of workers, $k$ denotes the number of data partitions.
\end{definition}

\begin{definition}
    Let $A \in \mathbb{R}^{f \times n}$, the $i^{th}$ row of $A$ be $\boldsymbol{a_i}$. Each row of $A$ indicates a combination of surviving workers/non-stragglers.
\end{definition}

\begin{definition}
    Let $B \in \mathbb{R}^{n \times k}$, the $i^{th}$ row of $B$ be $\boldsymbol{b_i}$. $\boldsymbol{b_i}$ indicates the data partitions that the $i^{th}$ worker has access to.
\end{definition}

\end{frame}

\begin{frame}{Gradient Coding}{The General Setup}

\begin{block}{Conditions to be Met}
    \[AB = \boldsymbol{1}_{f\times k}\]
\end{block}

\begin{columns}

\begin{column}{.5\linewidth}
        
\begin{Example}
    \[A = \begin{bmatrix}
        0 & 1 & 2 \\
        1 & 0 & 1 \\
        2 & -1 & 0
    \end{bmatrix}\]
    
    \[B = \begin{bmatrix}
        1/2 & 1 & 0 \\
        0 & 1 & -1 \\
        1/2 & 0 & 1
    \end{bmatrix}\]
\end{Example}
    
\end{column}

\begin{column}{.5\linewidth}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{res/gradient coding.pdf}
\end{figure}

\end{column}

\end{columns}

\end{frame}

\begin{frame}{Gradient Coding}{The General Setup}

\begin{block}{Property}
    \[\boldsymbol{a_i}B\boldsymbol{\bar{g}} = \begin{bmatrix}
        1 & 1 & \dots & 1
    \end{bmatrix}\boldsymbol{\bar{g}} = (\sum_{j=1}^{k}\boldsymbol{g_j})^T\]
    \[\boldsymbol{a_i}B\boldsymbol{\bar{g}} = \sum_{k\in supp(\boldsymbol{a_i})}\boldsymbol{a_i}(j)(\boldsymbol{b_j} \boldsymbol{\bar{g}})\]
    where $\boldsymbol{\bar{g}} = \begin{bmatrix}
        \boldsymbol{g_1} & \boldsymbol{g_2} & \dots & \boldsymbol{g_k}
    \end{bmatrix}^T$, $\boldsymbol{g_i}$ denotes the gradient of the $i^{th}$ partition of work, $supp(\boldsymbol{x}) = \{i | x_i \neq 0\}$.
\end{block}


\end{frame}

\section{Further Work}

\subsection{DRACO}

\begin{frame}{DRACO}

    \begin{itemize}
        \item L. Chen, H. Wang, Z. Charles, and D. Papailiopoulos, “\textbf{Draco: Byzantine-resilient distributed training via redundant gradients},” in International Conference on Machine Learning, PMLR, 2018, pp. 903–912.
    \end{itemize}

\end{frame}

\subsection{Interactive Gradient Coding}

\begin{frame}{Interactive Gradient Coding}

    \begin{itemize}
        \item C. Hofmeister, L. Maßny, E. Yaakobi, and R. Bitar, “\textbf{Trading Communication for Computation in Byzantine-Resilient Gradient Coding},” arXiv preprint arXiv:2303.13231, 2023.
    \end{itemize}

\end{frame}

\end{document}

